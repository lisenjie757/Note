{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19dbc65b",
   "metadata": {},
   "source": [
    "# K210从训练到部署实战"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde47ae",
   "metadata": {},
   "source": [
    "## 0. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f057e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pt12/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms, utils, models\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 若检测到有GPU则使用GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed6ed35",
   "metadata": {},
   "source": [
    "## 1. 各类辅助函数操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92523712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorg降采样操作\n",
    "class reorg_layer(nn.Module):\n",
    "    def __init__(self, stride):\n",
    "        super(reorg_layer, self).__init__()\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        _height, _width = height // self.stride, width // self.stride\n",
    "        \n",
    "        x = x.view(batch_size, channels, _height, self.stride, _width, self.stride).transpose(3, 4).contiguous()\n",
    "        x = x.view(batch_size, channels, _height * _width, self.stride * self.stride).transpose(2, 3).contiguous()\n",
    "        x = x.view(batch_size, channels, self.stride * self.stride, _height, _width).transpose(1, 2).contiguous()\n",
    "        x = x.view(batch_size, -1, _height, _width)\n",
    "\n",
    "        return x\n",
    "\n",
    "# iou分数计算\n",
    "def iou_score(bboxes_a, bboxes_b):\n",
    "    \"\"\"\n",
    "        bbox_1 : [B*N, 4] = [x1, y1, x2, y2]\n",
    "        bbox_2 : [B*N, 4] = [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    tl = torch.max(bboxes_a[:, :2], bboxes_b[:, :2])\n",
    "    br = torch.min(bboxes_a[:, 2:], bboxes_b[:, 2:])\n",
    "    area_a = torch.prod(bboxes_a[:, 2:] - bboxes_a[:, :2], 1)\n",
    "    area_b = torch.prod(bboxes_b[:, 2:] - bboxes_b[:, :2], 1)\n",
    "\n",
    "    en = (tl < br).type(tl.type()).prod(dim=1)\n",
    "    area_i = torch.prod(br - tl, 1) * en  # * ((tl < br).all())\n",
    "    return area_i / (area_a + area_b - area_i)\n",
    "\n",
    "# loss计算\n",
    "class MSEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MSEWithLogitsLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, targets, mask):\n",
    "        inputs = torch.clamp(torch.sigmoid(logits), min=1e-4, max=1.0 - 1e-4)\n",
    "\n",
    "        # 被忽略的先验框的mask都是-1，不参与loss计算\n",
    "        pos_id = (mask==1.0).float()\n",
    "        neg_id = (mask==0.0).float()\n",
    "        pos_loss = pos_id * (inputs - targets)**2\n",
    "        neg_loss = neg_id * (inputs)**2\n",
    "        loss = 5.0*pos_loss + 1.0*neg_loss\n",
    "\n",
    "        return loss\n",
    "def compute_loss(pred_conf, pred_cls, pred_txtytwth, targets):\n",
    "    batch_size = pred_conf.size(0)\n",
    "    # 损失函数\n",
    "    conf_loss_function = MSEWithLogitsLoss()\n",
    "    cls_loss_function = nn.CrossEntropyLoss(reduction='none')\n",
    "    txty_loss_function = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    twth_loss_function = nn.MSELoss(reduction='none')\n",
    "\n",
    "    # 预测\n",
    "    pred_conf = pred_conf[..., 0]           # [B, HW,]\n",
    "    pred_cls = pred_cls.permute(0, 2, 1)    # [B, C, HW]\n",
    "    pred_txty = pred_txtytwth[..., :2]      # [B, HW, 2]\n",
    "    pred_twth = pred_txtytwth[..., 2:]      # [B, HW, 2]\n",
    "\n",
    "    # 标签  \n",
    "    gt_conf = targets[..., 0].float()                 # [B, HW,]\n",
    "    gt_obj = targets[..., 1].float()                  # [B, HW,]\n",
    "    gt_cls = targets[..., 2].long()                   # [B, HW,]\n",
    "    gt_txty = targets[..., 3:5].float()               # [B, HW, 2]\n",
    "    gt_twth = targets[..., 5:7].float()               # [B, HW, 2]\n",
    "    gt_box_scale_weight = targets[..., 7]             # [B, HW,]\n",
    "    gt_mask = (gt_box_scale_weight > 0.).float()      # [B, HW,]\n",
    "\n",
    "    # 置信度损失\n",
    "    conf_loss = conf_loss_function(pred_conf, gt_conf, gt_obj)\n",
    "    conf_loss = conf_loss.sum() / batch_size\n",
    "    \n",
    "    # 类别损失\n",
    "    cls_loss = cls_loss_function(pred_cls, gt_cls) * gt_mask\n",
    "    cls_loss = cls_loss.sum() / batch_size\n",
    "    \n",
    "    # 边界框txty的损失\n",
    "    txty_loss = txty_loss_function(pred_txty, gt_txty).sum(-1) * gt_mask * gt_box_scale_weight\n",
    "    txty_loss = txty_loss.sum() / batch_size\n",
    "\n",
    "    # 边界框twth的损失\n",
    "    twth_loss = twth_loss_function(pred_twth, gt_twth).sum(-1) * gt_mask * gt_box_scale_weight\n",
    "    twth_loss = twth_loss.sum() / batch_size\n",
    "    bbox_loss = txty_loss + twth_loss\n",
    "\n",
    "    #总的损失\n",
    "    total_loss = conf_loss + cls_loss + bbox_loss\n",
    "\n",
    "    return conf_loss, cls_loss, bbox_loss, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c134afb",
   "metadata": {},
   "source": [
    "## 2. YOLO网络定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7d7f52",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (22223102.py, line 59)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [4], line 59\u001b[0;36m\u001b[0m\n\u001b[0;31m    def set_grid(self, input_size):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class YOLOv2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 cfg,\n",
    "                 device,\n",
    "                 input_size=416,\n",
    "                 num_classes=20,\n",
    "                 trainable=False,\n",
    "                 conf_thresh=0.001, \n",
    "                 nms_thresh=0.6, \n",
    "                 topk=100,\n",
    "                 anchor_size=None):\n",
    "        super(YOLOv2, self).__init__()\n",
    "        self.device = device              # cuda或是cpu \n",
    "        self.input_size = input_size      # 输入图像大小\n",
    "        self.num_classes = num_classes    # 类别数量\n",
    "        self.trainable = trainable        # 训练时标记\n",
    "        self.conf_thresh = conf_thresh    # 置信度阈值\n",
    "        self.nms_thresh = nms_thresh      # NMS阈值\n",
    "        self.stride = cfg['stride']       # 网络最大降采样倍数\n",
    "        self.topk = topk\n",
    "        \n",
    "        # Anchor box config\n",
    "        self.anchor_size = torch.tensor(anchor_size)  # [KA, 2]\n",
    "        self.num_anchors = len(anchor_size)\n",
    "        self.anchor_boxes = self.create_grid(input_size)  # 用于得到最终bbox的参数\n",
    "        \n",
    "        # CBR block\n",
    "        def conv(in_channels, out_channels, kernel_size, strides=1, padding=0):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding, bias=False),\n",
    "                nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True))\n",
    "        \n",
    "        # backbone\n",
    "        self.backbone = models.resnet50(pretrained=trainable)\n",
    "        c5 = 2048\n",
    "        \n",
    "        # neck\n",
    "        self.convsets_1 = nn.Sequential(\n",
    "            conv(c5, 1024, kernel_size=1),\n",
    "            conv(1024, 1024, kernel_size=3, strides=1, padding=1),\n",
    "            conv(1024, 1024, kernel_size=3, strides=1, padding=1))\n",
    "        \n",
    "        # 融合高分辨率的特征信息c4\n",
    "        self.route_layer = conv(1024, 128, kernel_size=1)\n",
    "        self.reorg = reorg_layer(stride=2)\n",
    "        \n",
    "        # head \n",
    "        self.convsets_2 = conv(1024+128*4, 1024, kernel_size=3, strides=1, padding=1)\n",
    "        \n",
    "        # prediction \n",
    "        self.pred = nn.Conv2d(1024, self.num_anchors*(1+4+self.num_classes), kernel_size=1)\n",
    "        \n",
    "        if self.trainable:\n",
    "            self.init_bias()\n",
    "\n",
    "\n",
    "    def init_bias(self):\n",
    "        # init bias\n",
    "        init_prob = 0.01\n",
    "        bias_value = -torch.log(torch.tensor((1. - init_prob) / init_prob))\n",
    "        nn.init.constant_(self.pred.bias[..., :self.num_anchors], bias_value)\n",
    "        nn.init.constant_(self.pred.bias[..., 1*self.num_anchors:(1+self.num_classes)*self.num_anchors], bias_value)\n",
    "\n",
    "    def create_grid(self, input_size):\n",
    "        # 生成一个tensor：grid_xy，每个位置的元素是网格的坐标，\n",
    "        # 这一tensor将在获得边界框参数的时候会用到。\n",
    "        w, h = input_size, input_size\n",
    "        # 生成G矩阵\n",
    "        fmp_w, fmp_h = w // self.stride, h // self.stride\n",
    "        grid_y, grid_x = torch.meshgrid([torch.arange(fmp_h), torch.arange(fmp_w)])\n",
    "        # [H, W, 2] -> [HW, 2]\n",
    "        grid_xy = torch.stack([grid_x, grid_y], dim=-1).float().view(-1, 2)\n",
    "        # [HW, 2] -> [HW, 1, 2] -> [HW, KA, 2]\n",
    "        grid_xy = grid_xy[:, None, :].repeat(1, self.num_anchors, 1)\n",
    "\n",
    "        # [KA, 2] -> [1, KA, 2] -> [HW, KA, 2]\n",
    "        anchor_wh = self.anchor_size[None, :, :].repeat(fmp_h*fmp_w, 1, 1)\n",
    "\n",
    "        # [HW, KA, 4] -> [M, 4]\n",
    "        anchor_boxes = torch.cat([grid_xy, anchor_wh], dim=-1)\n",
    "        anchor_boxes = anchor_boxes.view(-1, 4).to(self.device)\n",
    "\n",
    "        return anchor_boxes        \n",
    "\n",
    "\n",
    "    def set_grid(self, input_size):\n",
    "        # 用于重置grid_xy\n",
    "        self.input_size = input_size\n",
    "        self.anchor_boxes = self.create_grid(input_size)\n",
    "\n",
    "    def decode_boxes(self, anchors, txtytwth_pred):\n",
    "        # 将网络输出的tx,ty,tw,th四个量转换成bbox的(x1,y1),(x2,y2)\n",
    "        \"\"\"将txtytwth预测换算成边界框的左上角点坐标和右下角点坐标 \\n\n",
    "            Input: \\n\n",
    "                txtytwth_pred : [B, H*W*KA, 4] \\n\n",
    "            Output: \\n\n",
    "                x1y1x2y2_pred : [B, H*W*KA, 4] \\n\n",
    "        \"\"\"\n",
    "        # 获得边界框的中心点坐标和宽高\n",
    "        # b_x = sigmoid(tx) + gride_x\n",
    "        # b_y = sigmoid(ty) + gride_y\n",
    "        xy_pred = torch.sigmoid(txtytwth_pred[..., :2]) + anchors[..., :2]\n",
    "        # b_w = anchor_w * exp(tw)\n",
    "        # b_h = anchor_h * exp(th)\n",
    "        wh_pred = torch.exp(txtytwth_pred[..., 2:]) * anchors[..., 2:]\n",
    "\n",
    "        # [B, H*W*KA, 4]\n",
    "        xywh_pred = torch.cat([xy_pred, wh_pred], -1) * self.stride\n",
    "\n",
    "        # 将中心点坐标和宽高换算成边界框的左上角点坐标和右下角点坐标\n",
    "        x1y1x2y2_pred = torch.zeros_like(xywh_pred)\n",
    "        x1y1x2y2_pred[..., :2] = xywh_pred[..., :2] - xywh_pred[..., 2:] * 0.5\n",
    "        x1y1x2y2_pred[..., 2:] = xywh_pred[..., :2] + xywh_pred[..., 2:] * 0.5\n",
    "        \n",
    "        return x1y1x2y2_pred\n",
    "\n",
    "    def nms(self, dets, scores):\n",
    "        # 这是一个最基本的基于python语言的nms操作\n",
    "        # 这一代码来源于Faster RCNN项目\n",
    "        \"\"\"\"Pure Python NMS baseline.\"\"\"\n",
    "        x1 = dets[:, 0]  #xmin\n",
    "        y1 = dets[:, 1]  #ymin\n",
    "        x2 = dets[:, 2]  #xmax\n",
    "        y2 = dets[:, 3]  #ymax\n",
    "\n",
    "        areas = (x2 - x1) * (y2 - y1)                    # bbox的宽w和高h\n",
    "        order = scores.argsort()[::-1]                   # 按照降序对bbox的得分进行排序\n",
    "\n",
    "        keep = []                                        # 用于保存经过筛的最终bbox结果\n",
    "        while order.size > 0:\n",
    "            i = order[0]                                 # 得到最高的那个bbox\n",
    "            keep.append(i)                               \n",
    "            xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "            yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "            xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "            yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "            w = np.maximum(1e-28, xx2 - xx1)\n",
    "            h = np.maximum(1e-28, yy2 - yy1)\n",
    "            inter = w * h\n",
    "\n",
    "            # Cross Area / (bbox + particular area - Cross Area)\n",
    "            ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "            #reserve all the boundingbox whose ovr less than thresh\n",
    "            inds = np.where(ovr <= self.nms_thresh)[0]\n",
    "            order = order[inds + 1]\n",
    "\n",
    "        return keep\n",
    "\n",
    "\n",
    "    def postprocess(self, all_local, all_conf):\n",
    "        # 后处理代码\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            conf_pred: (Tensor) [H*W*KA, 1]\n",
    "            cls_pred:  (Tensor) [H*W*KA, C]\n",
    "            reg_pred:  (Tensor) [H*W*KA, 4]\n",
    "        \"\"\"\n",
    "        anchors = self.anchor_boxes\n",
    "\n",
    "        # (H x W x KA x C,)\n",
    "        scores = (torch.sigmoid(conf_pred) * torch.softmax(cls_pred, dim=-1)).flatten()\n",
    "\n",
    "        # Keep top k top scoring indices only.\n",
    "        num_topk = min(self.topk, reg_pred.size(0))\n",
    "\n",
    "        # torch.sort is actually faster than .topk (at least on GPUs)\n",
    "        predicted_prob, topk_idxs = scores.sort(descending=True)\n",
    "        topk_scores = predicted_prob[:num_topk]\n",
    "        topk_idxs = topk_idxs[:num_topk]\n",
    "\n",
    "        # filter out the proposals with low confidence score\n",
    "        keep_idxs = topk_scores > self.conf_thresh\n",
    "        scores = topk_scores[keep_idxs]\n",
    "        topk_idxs = topk_idxs[keep_idxs]\n",
    "\n",
    "        anchor_idxs = torch.div(topk_idxs, self.num_classes, rounding_mode='floor')\n",
    "        labels = topk_idxs % self.num_classes\n",
    "\n",
    "        reg_pred = reg_pred[anchor_idxs]\n",
    "        anchors = anchors[anchor_idxs]\n",
    "\n",
    "        # 解算边界框, 并归一化边界框: [H*W*KA, 4]\n",
    "        bboxes = self.decode_boxes(anchors, reg_pred)\n",
    "        \n",
    "        # to cpu\n",
    "        scores = scores.cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "        bboxes = bboxes.cpu().numpy()\n",
    "\n",
    "        # NMS\n",
    "        keep = np.zeros(len(bboxes), dtype=np.int)\n",
    "        for i in range(self.num_classes):\n",
    "            inds = np.where(labels == i)[0]\n",
    "            if len(inds) == 0:\n",
    "                continue\n",
    "            c_bboxes = bboxes[inds]\n",
    "            c_scores = scores[inds]\n",
    "            c_keep = self.nms(c_bboxes, c_scores)\n",
    "            keep[inds[c_keep]] = 1\n",
    "\n",
    "        keep = np.where(keep > 0)\n",
    "        bboxes = bboxes[keep]\n",
    "        scores = scores[keep]\n",
    "        labels = labels[keep]\n",
    "\n",
    "        # 归一化边界框\n",
    "        bboxes = bboxes / self.input_size\n",
    "        bboxes = np.clip(bboxes, 0., 1.)\n",
    "\n",
    "        return bboxes, scores, labels\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def inference(self, x):\n",
    "        # backbone\n",
    "        _, c4, c5 = self.backbone(x)\n",
    "\n",
    "        # neck\n",
    "        p5 = self.convsets_1(c5)\n",
    "        ## 处理c4特征\n",
    "        p4 = self.reorg(self.route_layer(c4))\n",
    "        ## 融合c4特征\n",
    "        p5 = torch.cat([p4,p5], dim=1)\n",
    "\n",
    "        # head\n",
    "        p5 = self.convsets_2(p5)\n",
    "\n",
    "        # prediction\n",
    "        prediction = self.pred(p5)\n",
    "\n",
    "        B, abC, H, W = prediction.size()\n",
    "        KA = self.num_anchors\n",
    "        NC = self.num_classes\n",
    "\n",
    "        # [B, KA * C, H, W] -> [B, H, W, KA * C] -> [B, H*W, KA*C]\n",
    "        prediction = prediction.permute(0, 2, 3, 1).contiguous().view(B, H*W, abC)\n",
    "\n",
    "        # 从pred中分离出objectness预测、类别class预测、bbox的txtytwth预测  \n",
    "        # [B, H*W, KA*C] -> [B, H*W, KA] -> [B, H*W*KA, 1]\n",
    "        conf_pred = prediction[..., :KA].contiguous().view(B, -1, 1)\n",
    "        # [B, H*W, KA*C] -> [B, H*W, KA*NC] -> [B, H*W*KA, NC]\n",
    "        cls_pred = prediction[..., 1*KA : (1+NC)*KA].contiguous().view(B, -1, NC)\n",
    "        # [B, H*W, KA*C] -> [B, H*W, KA*4] -> [B, H*W*KA, 4]\n",
    "        txtytwth_pred = prediction[..., (1+NC)*KA:].contiguous().view(B, -1, 4) \n",
    "        \n",
    "        # 测试时，默认batch为1，\n",
    "        # 因此，我们不需要用batch这个维度，用[0]将其取走。\n",
    "        conf_pred = conf_pred[0]            #[H*W*KA, 1]\n",
    "        cls_pred = cls_pred[0]              #[H*W*KA, NC]\n",
    "        txtytwth_pred = txtytwth_pred[0]    #[H*W*KA, 4]\n",
    "\n",
    "        # 后处理\n",
    "        bboxes, scores, labels = self.postprocess(conf_pred, cls_pred, txtytwth_pred)\n",
    "\n",
    "        return bboxes, scores, labels\n",
    "    \n",
    "    def forward(self, x, target=None):\n",
    "        # 前向推理的代码，主要分为两部分：\n",
    "        # 训练部分：网络得到obj、cls和txtytwth三个分支的预测，然后计算loss；\n",
    "        # 推理部分：输出经过后处理得到的bbox、cls和每个bbox的预测得分。\n",
    "        if not self.trainable:\n",
    "            return self.inference(x)\n",
    "        else:\n",
    "            # backbone\n",
    "            _, c4, c5 = self.backbone(x)\n",
    "\n",
    "            # neck\n",
    "            p5 = self.convsets_1(c5)\n",
    "            ## 处理c4特征\n",
    "            p4 = self.reorg(self.route_layer(c4))\n",
    "            ## 融合c4特征\n",
    "            p5 = torch.cat([p4,p5], dim=1)\n",
    "            \n",
    "            # head\n",
    "            p5 = self.convsets_2(p5)\n",
    "            \n",
    "            # prediction\n",
    "            prediction = self.pred(p5)\n",
    "            \n",
    "            B, abC, H, W = prediction.size()\n",
    "            KA = self.num_anchors\n",
    "            NC = self.num_classes\n",
    "\n",
    "            # [B, KA * C, H, W] -> [B, H, W, KA * C] -> [B, H*W, KA*C]\n",
    "            prediction = prediction.permute(0, 2, 3, 1).contiguous().view(B, H*W, abC)\n",
    "\n",
    "            # 从pred中分离出objectness预测、类别class预测、bbox的txtytwth预测  \n",
    "            # [B, H*W, KA*C] -> [B, H*W, KA] -> [B, H*W*KA, 1]\n",
    "            conf_pred = prediction[..., :KA].contiguous().view(B, -1, 1)\n",
    "            # [B, H*W, KA*C] -> [B, H*W, KA*NC] -> [B, H*W*KA, NC]\n",
    "            cls_pred = prediction[..., 1*KA : (1+NC)*KA].contiguous().view(B, -1, NC)\n",
    "            # [B, H*W, KA*C] -> [B, H*W, KA*4] -> [B, H*W*KA, 4]\n",
    "            txtytwth_pred = prediction[..., (1+NC)*KA:].contiguous().view(B, -1, 4) \n",
    "            \n",
    "            # 添加IoU Loss进入计算\n",
    "            ## 解算边界框\n",
    "            x1y1x2y2_pred = (self.decode_boxes(self.anchor_boxes, txtytwth_pred) / self.input_size).view(-1, 4)\n",
    "            x1y1x2y2_gt = targets[:, :, 7:].view(-1, 4)\n",
    "            ## 计算预测框和真实框之间的IoU\n",
    "            iou_pred = iou_score(x1y1x2y2_pred, x1y1x2y2_gt).view(B, -1, 1)\n",
    "            ## 将IoU作为置信度的学习目标\n",
    "            with torch.no_grad():\n",
    "                gt_conf = iou_pred.clone()\n",
    "            ## 将IoU作为置信度的学习目标 \n",
    "            ## [obj, cls, txtytwth, x1y1x2y2] -> [conf, obj, cls, txtytwth]\n",
    "            targets = torch.cat([gt_conf, targets[:, :, :7]], dim=2)\n",
    "            \n",
    "            # 计算损失\n",
    "            (\n",
    "                conf_loss,\n",
    "                cls_loss,\n",
    "                bbox_loss,\n",
    "                total_loss\n",
    "            ) = compute_loss(\n",
    "                pred_conf=conf_pred, \n",
    "                pred_cls=cls_pred,\n",
    "                pred_txtytwth=txtytwth_pred,\n",
    "                targets=targets,\n",
    "                )\n",
    "\n",
    "            return conf_loss, cls_loss, bbox_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31665f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start trainig, training on cuda\n",
      "use the multi-scale trick ...\n",
      "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar to ../data/VOCtrainval_06-Nov-2007.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████| 460032000/460032000 [02:46<00:00, 2759454.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/VOCtrainval_06-Nov-2007.tar to ../data\n",
      "Using downloaded and verified file: ../data/VOCtrainval_06-Nov-2007.tar\n",
      "Extracting ../data/VOCtrainval_06-Nov-2007.tar to ../data\n"
     ]
    }
   ],
   "source": [
    "# 超参数设置\n",
    "epochs = 20\n",
    "batch_size_train = 64\n",
    "batch_size_test = 64\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "\n",
    "# 训练配置\n",
    "num_workers = 12\n",
    "log_interval = 10\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# 保存模型的路径\n",
    "#path_to_save = os.path.join(args.save_folder, args.dataset, args.version)\n",
    "#os.makedirs(path_to_save, exist_ok=True)\n",
    "\n",
    "print('start trainig, training on', device)\n",
    "\n",
    "# 使用多尺度训练\n",
    "print('use the multi-scale trick ...')\n",
    "train_size = 640\n",
    "val_size = 416\n",
    "\n",
    "# 构建yolov2的配置文件\n",
    "cfg = {\n",
    "        # model\n",
    "        'backbone': 'darknet19',\n",
    "        'pretrained': True,\n",
    "        'stride': 32,  # P5\n",
    "        'reorg_dim': 64,\n",
    "        'head_dim': 1024,\n",
    "        # anchor size\n",
    "        'anchor_size': {\n",
    "            'voc': [[1.19, 1.98], [2.79, 4.59], [4.53, 8.92], [8.06, 5.29], [10.32, 10.65]],\n",
    "            'coco': [[0.53, 0.79], [1.71, 2.36], [2.89, 6.44], [6.33, 3.79], [9.03, 9.74]]\n",
    "            },\n",
    "        # matcher\n",
    "        'ignore_thresh': 0.5,\n",
    "        }\n",
    "\n",
    "# 构建dataset类\n",
    "train_datasets = datasets.VOCDetection(root='../data', year='2007', image_set='train', download=True,\n",
    "                     transform=transforms.Compose([\n",
    "                         transforms.Resize((224,224)),\n",
    "                         transforms.ToTensor(),]))\n",
    "test_datasets = datasets.VOCDetection(root='../data', year=\"2007\", image_set='val', download=True,\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.Resize((224,224)),\n",
    "                          transforms.ToTensor(),]))\n",
    "# 由于标注长度不等，需要构建数据采样函数\n",
    "def detection_collate(batch):\n",
    "    \"\"\"Custom collate fn for dealing with batches of images that have a different\n",
    "    number of associated object annotations (bounding boxes).\n",
    "    Arguments:\n",
    "        batch: (tuple) A tuple of tensor images and lists of annotations\n",
    "    Return:\n",
    "        A tuple containing:\n",
    "            1) (tensor) batch of images stacked on their 0 dim\n",
    "            2) (list of tensors) annotations for a given image are stacked on\n",
    "                                 0 dim\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    imgs = []\n",
    "    for sample in batch:\n",
    "        imgs.append(sample[0])\n",
    "        print(\"------------\")\n",
    "        print(sample[0].shape)\n",
    "        print(sample[1])\n",
    "        targets.append(torch.FloatTensor(sample[1]))\n",
    "    return torch.stack(imgs, 0), targets\n",
    "# 构建dataloader类\n",
    "train_dataloader = DataLoader(train_datasets, batch_size=batch_size_train, shuffle=True, num_workers=num_workers,\n",
    "                             collate_fn=detection_collate, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_datasets, batch_size=batch_size_test, shuffle=True, num_workers=num_workers,\n",
    "                            collate_fn=detection_collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c4c86bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "torch.Size([3, 224, 224])------------\n",
      "{'annotation': {'folder': 'VOC2012', 'filename': '2008_007025.jpg', 'source': {'database': 'The VOC2008 Database', 'annotation': 'PASCAL VOC2008', 'image': 'flickr'}, 'size': {'width': '500', 'height': '333', 'depth': '3'}, 'segmented': '1', 'object': [{'name': 'cow', 'pose': 'Left', 'truncated': '0', 'occluded': '0', 'bndbox': {'xmin': '212', 'ymin': '128', 'xmax': '470', 'ymax': '305'}, 'difficult': '0'}, {'name': 'cow', 'pose': 'Left', 'truncated': '0', 'occluded': '1', 'bndbox': {'xmin': '122', 'ymin': '107', 'xmax': '393', 'ymax': '291'}, 'difficult': '0'}]}}\n",
      "\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'filename': '2009_005202.jpg', 'folder': 'VOC2012', 'object': [{'name': 'diningtable', 'bndbox': {'xmax': '371', 'xmin': '1', 'ymax': '375', 'ymin': '90'}, 'difficult': '0', 'occluded': '1', 'pose': 'Unspecified', 'truncated': '0'}, {'name': 'chair', 'bndbox': {'xmax': '500', 'xmin': '322', 'ymax': '375', 'ymin': '227'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '1'}, {'name': 'chair', 'bndbox': {'xmax': '250', 'xmin': '30', 'ymax': '375', 'ymin': '168'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '1'}, {'name': 'chair', 'bndbox': {'xmax': '321', 'xmin': '250', 'ymax': '102', 'ymin': '53'}, 'difficult': '0', 'occluded': '0', 'pose': 'Frontal', 'truncated': '1'}, {'name': 'chair', 'bndbox': {'xmax': '430', 'xmin': '278', 'ymax': '291', 'ymin': '102'}, 'difficult': '0', 'occluded': '1', 'pose': 'Unspecified', 'truncated': '1'}, {'name': 'chair', 'bndbox': {'xmax': '111', 'xmin': '28', 'ymax': '160', 'ymin': '69'}, 'difficult': '0', 'occluded': '1', 'pose': 'Unspecified', 'truncated': '1'}], 'segmented': '0', 'size': {'depth': '3', 'height': '375', 'width': '500'}, 'source': {'annotation': 'PASCAL VOC2009', 'database': 'The VOC2009 Database', 'image': 'flickr'}}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'filename': '2010_000870.jpg', 'folder': 'VOC2012', 'object': [{'name': 'dog', 'bndbox': {'xmax': '445', 'xmin': '251', 'ymax': '320', 'ymin': '50'}, 'difficult': '0', 'occluded': '0', 'pose': 'Frontal', 'truncated': '0'}], 'segmented': '0', 'size': {'depth': '3', 'height': '375', 'width': '500'}, 'source': {'annotation': 'PASCAL VOC2010', 'database': 'The VOC2010 Database', 'image': 'flickr'}}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'filename': '2011_002740.jpg', 'folder': 'VOC2012', 'object': [{'name': 'boat', 'bndbox': {'xmax': '380', 'xmin': '306', 'ymax': '165', 'ymin': '138'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '0'}, {'name': 'boat', 'bndbox': {'xmax': '472', 'xmin': '341', 'ymax': '248', 'ymin': '212'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '0'}, {'name': 'boat', 'bndbox': {'xmax': '247', 'xmin': '202', 'ymax': '122', 'ymin': '107'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '0'}, {'name': 'boat', 'bndbox': {'xmax': '469', 'xmin': '422', 'ymax': '100', 'ymin': '34'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '0'}, {'name': 'boat', 'bndbox': {'xmax': '95', 'xmin': '67', 'ymax': '117', 'ymin': '100'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '0'}, {'name': 'boat', 'bndbox': {'xmax': '373', 'xmin': '344', 'ymax': '104', 'ymin': '85'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '0'}, {'name': 'boat', 'bndbox': {'xmax': '60', 'xmin': '35', 'ymax': '119', 'ymin': '90'}, 'difficult': '0', 'occluded': '0', 'pose': 'Frontal', 'truncated': '0'}, {'name': 'boat', 'bndbox': {'xmax': '215', 'xmin': '183', 'ymax': '116', 'ymin': '100'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '0'}, {'name': 'boat', 'bndbox': {'xmax': '428', 'xmin': '403', 'ymax': '99', 'ymin': '85'}, 'difficult': '1', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '0'}, {'name': 'boat', 'bndbox': {'xmax': '427', 'xmin': '274', 'ymax': '237', 'ymin': '206'}, 'difficult': '0', 'occluded': '1', 'pose': 'Right', 'truncated': '0'}, {'name': 'boat', 'bndbox': {'xmax': '438', 'xmin': '331', 'ymax': '315', 'ymin': '254'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '0'}, {'name': 'boat', 'bndbox': {'xmax': '342', 'xmin': '192', 'ymax': '280', 'ymin': '236'}, 'difficult': '0', 'occluded': '1', 'pose': 'Unspecified', 'truncated': '0'}, {'name': 'boat', 'bndbox': {'xmax': '336', 'xmin': '85', 'ymax': '301', 'ymin': '234'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '0'}, {'name': 'boat', 'bndbox': {'xmax': '64', 'xmin': '1', 'ymax': '295', 'ymin': '221'}, 'difficult': '1', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '1'}, {'name': 'boat', 'bndbox': {'xmax': '158', 'xmin': '1', 'ymax': '281', 'ymin': '216'}, 'difficult': '0', 'occluded': '1', 'pose': 'Right', 'truncated': '1'}, {'name': 'boat', 'bndbox': {'xmax': '500', 'xmin': '338', 'ymax': '305', 'ymin': '250'}, 'difficult': '1', 'occluded': '1', 'pose': 'Unspecified', 'truncated': '1'}, {'name': 'person', 'bndbox': {'xmax': '373', 'xmin': '350', 'ymax': '154', 'ymin': '128'}, 'difficult': '1', 'occluded': '1', 'pose': 'Unspecified', 'truncated': '1'}], 'segmented': '0', 'size': {'depth': '3', 'height': '333', 'width': '500'}, 'source': {'annotation': 'PASCAL VOC2011', 'database': 'The VOC2011 Database', 'image': 'flickr'}}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'folder': 'VOC2012', 'filename': '2008_002452.jpg', 'source': {'database': 'The VOC2008 Database', 'annotation': 'PASCAL VOC2008', 'image': 'flickr'}, 'size': {'width': '500', 'height': '375', 'depth': '3'}, 'segmented': '0', 'object': [{'name': 'train', 'pose': 'Rear', 'truncated': '0', 'occluded': '0', 'bndbox': {'xmin': '136', 'ymin': '4', 'xmax': '251', 'ymax': '106'}, 'difficult': '0'}, {'name': 'train', 'pose': 'Unspecified', 'truncated': '1', 'occluded': '1', 'bndbox': {'xmin': '285', 'ymin': '36', 'xmax': '500', 'ymax': '72'}, 'difficult': '0'}]}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'filename': '2010_003326.jpg', 'folder': 'VOC2012', 'object': [{'name': 'person', 'bndbox': {'xmax': '427', 'xmin': '9', 'ymax': '500', 'ymin': '21'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '0', 'part': [{'name': 'head', 'bndbox': {'xmin': '103', 'ymin': '18', 'xmax': '249', 'ymax': '200'}}, {'name': 'hand', 'bndbox': {'xmin': '6', 'ymin': '302', 'xmax': '60', 'ymax': '390'}}, {'name': 'foot', 'bndbox': {'xmin': '115', 'ymin': '429', 'xmax': '232', 'ymax': '498'}}, {'name': 'foot', 'bndbox': {'xmin': '280', 'ymin': '385', 'xmax': '421', 'ymax': '480'}}]}], 'segmented': '0', 'size': {'depth': '3', 'height': '500', 'width': '450'}, 'source': {'annotation': 'PASCAL VOC2010', 'database': 'The VOC2010 Database', 'image': 'flickr'}}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'filename': '2009_004828.jpg', 'folder': 'VOC2012', 'object': [{'name': 'dog', 'bndbox': {'xmax': '381', 'xmin': '126', 'ymax': '388', 'ymin': '1'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '0'}], 'segmented': '0', 'size': {'depth': '3', 'height': '416', 'width': '500'}, 'source': {'annotation': 'PASCAL VOC2009', 'database': 'The VOC2009 Database', 'image': 'flickr'}}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'folder': 'VOC2012', 'filename': '2008_001374.jpg', 'source': {'database': 'The VOC2008 Database', 'annotation': 'PASCAL VOC2008', 'image': 'flickr'}, 'size': {'width': '500', 'height': '313', 'depth': '3'}, 'segmented': '0', 'object': [{'name': 'cat', 'pose': 'Right', 'truncated': '0', 'occluded': '0', 'bndbox': {'xmin': '1', 'ymin': '28', 'xmax': '492', 'ymax': '280'}, 'difficult': '0'}]}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'filename': '2010_004355.jpg', 'folder': 'VOC2012', 'object': [{'name': 'motorbike', 'bndbox': {'xmax': '441', 'xmin': '130', 'ymax': '344', 'ymin': '1'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '1'}], 'segmented': '1', 'size': {'depth': '3', 'height': '375', 'width': '500'}, 'source': {'annotation': 'PASCAL VOC2010', 'database': 'The VOC2010 Database', 'image': 'flickr'}}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'filename': '2009_000214.jpg', 'folder': 'VOC2012', 'object': [{'name': 'tvmonitor', 'bndbox': {'xmax': '228', 'xmin': '179', 'ymax': '173', 'ymin': '109'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '0'}, {'name': 'chair', 'bndbox': {'xmax': '348', 'xmin': '230', 'ymax': '333', 'ymin': '179'}, 'difficult': '0', 'occluded': '0', 'pose': 'Left', 'truncated': '0'}, {'name': 'pottedplant', 'bndbox': {'xmax': '291', 'xmin': '257', 'ymax': '197', 'ymin': '148'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '0'}], 'segmented': '0', 'size': {'depth': '3', 'height': '333', 'width': '500'}, 'source': {'annotation': 'PASCAL VOC2009', 'database': 'The VOC2009 Database', 'image': 'flickr'}}}\n",
      "------------------------\n",
      "torch.Size([3, 224, 224])\n",
      "\n",
      "torch.Size([3, 224, 224]){'annotation': {'folder': 'VOC2012', 'filename': '2008_003621.jpg', 'source': {'database': 'The VOC2008 Database', 'annotation': 'PASCAL VOC2008', 'image': 'flickr'}, 'size': {'width': '500', 'height': '375', 'depth': '3'}, 'segmented': '0', 'object': [{'name': 'bottle', 'pose': 'Unspecified', 'truncated': '1', 'occluded': '0', 'bndbox': {'xmin': '458', 'ymin': '151', 'xmax': '472', 'ymax': '198'}, 'difficult': '0'}]}}\n",
      "\n",
      "{'annotation': {'folder': 'VOC2012', 'filename': '2008_000553.jpg', 'source': {'database': 'The VOC2008 Database', 'annotation': 'PASCAL VOC2008', 'image': 'flickr'}, 'size': {'width': '413', 'height': '500', 'depth': '3'}, 'segmented': '0', 'object': [{'name': 'person', 'pose': 'Unspecified', 'truncated': '1', 'occluded': '1', 'bndbox': {'xmin': '1', 'ymin': '3', 'xmax': '302', 'ymax': '470'}, 'difficult': '0'}, {'name': 'person', 'pose': 'Unspecified', 'truncated': '1', 'occluded': '0', 'bndbox': {'xmin': '67', 'ymin': '298', 'xmax': '413', 'ymax': '499'}, 'difficult': '0'}, {'name': 'person', 'pose': 'Unspecified', 'truncated': '1', 'occluded': '1', 'bndbox': {'xmin': '1', 'ymin': '309', 'xmax': '287', 'ymax': '499'}, 'difficult': '0'}]}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'filename': '2010_002137.jpg', 'folder': 'VOC2012', 'object': [{'name': 'cat', 'bndbox': {'xmax': '282', 'xmin': '160', 'ymax': '312', 'ymin': '189'}, 'difficult': '0', 'occluded': '0', 'pose': 'Left', 'truncated': '0'}, {'name': 'sofa', 'bndbox': {'xmax': '375', 'xmin': '27', 'ymax': '500', 'ymin': '269'}, 'difficult': '1', 'occluded': '1', 'pose': 'Unspecified', 'truncated': '1'}, {'name': 'person', 'bndbox': {'xmax': '74', 'xmin': '1', 'ymax': '500', 'ymin': '94'}, 'difficult': '1', 'occluded': '0', 'pose': 'Frontal', 'truncated': '1'}, {'name': 'pottedplant', 'bndbox': {'xmax': '371', 'xmin': '52', 'ymax': '320', 'ymin': '1'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '1'}], 'segmented': '1', 'size': {'depth': '3', 'height': '500', 'width': '375'}, 'source': {'annotation': 'PASCAL VOC2010', 'database': 'The VOC2010 Database', 'image': 'flickr'}}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'filename': '2009_004607.jpg', 'folder': 'VOC2012', 'object': [{'name': 'cat', 'bndbox': {'xmax': '405', 'xmin': '71', 'ymax': '333', 'ymin': '1'}, 'difficult': '0', 'occluded': '0', 'pose': 'Frontal', 'truncated': '1'}], 'segmented': '0', 'size': {'depth': '3', 'height': '333', 'width': '500'}, 'source': {'annotation': 'PASCAL VOC2009', 'database': 'The VOC2009 Database', 'image': 'flickr'}}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'filename': '2010_002661.jpg', 'folder': 'VOC2012', 'object': [{'name': 'pottedplant', 'bndbox': {'xmax': '372', 'xmin': '15', 'ymax': '460', 'ymin': '4'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '0'}], 'segmented': '0', 'size': {'depth': '3', 'height': '500', 'width': '375'}, 'source': {'annotation': 'PASCAL VOC2010', 'database': 'The VOC2010 Database', 'image': 'flickr'}}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'folder': 'VOC2012', 'filename': '2008_001830.jpg', 'source': {'database': 'The VOC2008 Database', 'annotation': 'PASCAL VOC2008', 'image': 'flickr'}, 'size': {'width': '500', 'height': '375', 'depth': '3'}, 'segmented': '0', 'object': [{'name': 'car', 'pose': 'Right', 'truncated': '1', 'occluded': '0', 'bndbox': {'xmin': '288', 'ymin': '156', 'xmax': '500', 'ymax': '334'}, 'difficult': '0'}, {'name': 'car', 'pose': 'Right', 'truncated': '1', 'occluded': '0', 'bndbox': {'xmin': '130', 'ymin': '150', 'xmax': '299', 'ymax': '226'}, 'difficult': '0'}, {'name': 'car', 'pose': 'Frontal', 'truncated': '0', 'occluded': '0', 'bndbox': {'xmin': '1', 'ymin': '156', 'xmax': '51', 'ymax': '194'}, 'difficult': '0'}]}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'filename': '2009_004001.jpg', 'folder': 'VOC2012', 'object': [{'name': 'bird', 'bndbox': {'xmax': '435', 'xmin': '61', 'ymax': '333', 'ymin': '11'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '1'}], 'segmented': '0', 'size': {'depth': '3', 'height': '333', 'width': '500'}, 'source': {'annotation': 'PASCAL VOC2009', 'database': 'The VOC2009 Database', 'image': 'flickr'}}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'filename': '2009_003376.jpg', 'folder': 'VOC2012', 'object': [{'name': 'car', 'bndbox': {'xmax': '495', 'xmin': '13', 'ymax': '316', 'ymin': '35'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '0'}, {'name': 'car', 'bndbox': {'xmax': '500', 'xmin': '279', 'ymax': '206', 'ymin': '45'}, 'difficult': '0', 'occluded': '1', 'pose': 'Unspecified', 'truncated': '1'}], 'segmented': '0', 'size': {'depth': '3', 'height': '357', 'width': '500'}, 'source': {'annotation': 'PASCAL VOC2009', 'database': 'The VOC2009 Database', 'image': 'flickr'}}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'folder': 'VOC2012', 'filename': '2008_002876.jpg', 'source': {'database': 'The VOC2008 Database', 'annotation': 'PASCAL VOC2008', 'image': 'flickr'}, 'size': {'width': '500', 'height': '375', 'depth': '3'}, 'segmented': '0', 'object': [{'name': 'pottedplant', 'pose': 'Unspecified', 'truncated': '0', 'occluded': '0', 'bndbox': {'xmin': '75', 'ymin': '330', 'xmax': '109', 'ymax': '360'}, 'difficult': '0'}]}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'filename': '2009_001024.jpg', 'folder': 'VOC2012', 'object': [{'name': 'person', 'bndbox': {'xmax': '327', 'xmin': '140', 'ymax': '375', 'ymin': '107'}, 'difficult': '0', 'occluded': '1', 'pose': 'Unspecified', 'truncated': '1'}, {'name': 'bottle', 'bndbox': {'xmax': '297', 'xmin': '207', 'ymax': '217', 'ymin': '154'}, 'difficult': '0', 'occluded': '1', 'pose': 'Unspecified', 'truncated': '0'}, {'name': 'bottle', 'bndbox': {'xmax': '492', 'xmin': '449', 'ymax': '375', 'ymin': '316'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '1'}], 'segmented': '0', 'size': {'depth': '3', 'height': '375', 'width': '500'}, 'source': {'annotation': 'PASCAL VOC2009', 'database': 'The VOC2009 Database', 'image': 'flickr'}}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annotation': {'filename': '2011_000206.jpg', 'folder': 'VOC2012', 'object': [{'name': 'bottle', 'bndbox': {'xmax': '414', 'xmin': '374', 'ymax': '371', 'ymin': '228'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '0'}, {'name': 'bottle', 'bndbox': {'xmax': '205', 'xmin': '158', 'ymax': '375', 'ymin': '246'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '1'}, {'name': 'diningtable', 'bndbox': {'xmax': '500', 'xmin': '18', 'ymax': '375', 'ymin': '344'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '1'}, {'name': 'person', 'bndbox': {'xmax': '455', 'xmin': '240', 'ymax': '356', 'ymin': '95'}, 'difficult': '0', 'occluded': '1', 'pose': 'Unspecified', 'truncated': '1'}, {'name': 'person', 'bndbox': {'xmax': '219', 'xmin': '1', 'ymax': '375', 'ymin': '115'}, 'difficult': '0', 'occluded': '1', 'pose': 'Frontal', 'truncated': '1'}], 'segmented': '0', 'size': {'depth': '3', 'height': '375', 'width': '500'}, 'source': {'annotation': 'PASCAL VOC2011', 'database': 'The VOC2011 Database', 'image': 'flickr'}}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'filename': '2011_002158.jpg', 'folder': 'VOC2012', 'object': [{'name': 'tvmonitor', 'bndbox': {'xmax': '41', 'xmin': '2', 'ymax': '318', 'ymin': '96'}, 'difficult': '1', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '1'}, {'name': 'person', 'bndbox': {'xmax': '499', 'xmin': '203', 'ymax': '375', 'ymin': '31'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '1'}, {'name': 'bottle', 'bndbox': {'xmax': '160', 'xmin': '117', 'ymax': '357', 'ymin': '245'}, 'difficult': '0', 'occluded': '1', 'pose': 'Unspecified', 'truncated': '1'}, {'name': 'bottle', 'bndbox': {'xmax': '235', 'xmin': '179', 'ymax': '306', 'ymin': '258'}, 'difficult': '0', 'occluded': '1', 'pose': 'Unspecified', 'truncated': '0'}, {'name': 'bottle', 'bndbox': {'xmax': '245', 'xmin': '205', 'ymax': '314', 'ymin': '263'}, 'difficult': '0', 'occluded': '1', 'pose': 'Unspecified', 'truncated': '1'}], 'segmented': '0', 'size': {'depth': '3', 'height': '375', 'width': '500'}, 'source': {'annotation': 'PASCAL VOC2011', 'database': 'The VOC2011 Database', 'image': 'flickr'}}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'filename': '2010_000349.jpg', 'folder': 'VOC2012', 'object': [{'name': 'chair', 'bndbox': {'xmax': '112', 'xmin': '72', 'ymax': '278', 'ymin': '198'}, 'difficult': '0', 'occluded': '0', 'pose': 'Left', 'truncated': '0'}], 'segmented': '0', 'size': {'depth': '3', 'height': '374', 'width': '500'}, 'source': {'annotation': 'PASCAL VOC2010', 'database': 'The VOC2010 Database', 'image': 'flickr'}}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'folder': 'VOC2012', 'filename': '2008_002430.jpg', 'source': {'database': 'The VOC2008 Database', 'annotation': 'PASCAL VOC2008', 'image': 'flickr'}, 'size': {'width': '500', 'height': '328', 'depth': '3'}, 'segmented': '0', 'object': [{'name': 'dog', 'pose': 'Frontal', 'truncated': '0', 'occluded': '1', 'bndbox': {'xmin': '31', 'ymin': '116', 'xmax': '94', 'ymax': '212'}, 'difficult': '0'}, {'name': 'sheep', 'pose': 'Frontal', 'truncated': '0', 'occluded': '0', 'bndbox': {'xmin': '418', 'ymin': '60', 'xmax': '496', 'ymax': '258'}, 'difficult': '0'}, {'name': 'sheep', 'pose': 'Frontal', 'truncated': '0', 'occluded': '1', 'bndbox': {'xmin': '370', 'ymin': '106', 'xmax': '424', 'ymax': '243'}, 'difficult': '0'}, {'name': 'sheep', 'pose': 'Frontal', 'truncated': '0', 'occluded': '0', 'bndbox': {'xmin': '303', 'ymin': '65', 'xmax': '379', 'ymax': '257'}, 'difficult': '0'}]}}\n",
      "------------\n",
      "torch.Size([3, 224, 224])\n",
      "{'annotation': {'filename': '2011_000057.jpg', 'folder': 'VOC2012', 'object': [{'name': 'cow', 'bndbox': {'xmax': '332', 'xmin': '66', 'ymax': '340', 'ymin': '34'}, 'difficult': '0', 'occluded': '0', 'pose': 'Frontal', 'truncated': '0'}, {'name': 'cow', 'bndbox': {'xmax': '488', 'xmin': '269', 'ymax': '341', 'ymin': '75'}, 'difficult': '0', 'occluded': '0', 'pose': 'Unspecified', 'truncated': '1'}], 'segmented': '0', 'size': {'depth': '3', 'height': '364', 'width': '500'}, 'source': {'annotation': 'PASCAL VOC2011', 'database': 'The VOC2011 Database', 'image': 'flickr'}}}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/miniconda3/envs/pt12/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/miniconda3/envs/pt12/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/tmp/ipykernel_12944/86472786.py\", line 70, in detection_collate\n    targets.append(torch.FloatTensor(sample[1]))\nTypeError: new(): data must be a sequence (got dict)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28menumerate\u001b[39m(test_dataloader)\n\u001b[0;32m----> 2\u001b[0m batch_idx, (example_data, example_targets) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(example_targets\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(example_data\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pt12/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pt12/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1376\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pt12/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1402\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pt12/lib/python3.9/site-packages/torch/_utils.py:461\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/miniconda3/envs/pt12/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/miniconda3/envs/pt12/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/tmp/ipykernel_12944/86472786.py\", line 70, in detection_collate\n    targets.append(torch.FloatTensor(sample[1]))\nTypeError: new(): data must be a sequence (got dict)\n"
     ]
    }
   ],
   "source": [
    "examples = enumerate(test_dataloader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_targets.shape)\n",
    "print(example_data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt12",
   "language": "python",
   "name": "pt12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# 1.【经典综述】模型压缩和硬件加速综述 论文精读

> 原文链接：[Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey](https://ieeexplore.ieee.org/abstract/document/9043731)

## 0. 核心总结

核心是提出了一种**残差结构**，极好地解决了网络精度随着深度的增加而下降的问题，使得可以通过堆叠层数的方式来提升精度。

## 1. 摘要

更深的神经网络更难训练，本文提出了一个残差学习框架，以减轻深层神经网络训练的难度。本文将网络的层重构为引入层输入的可学习残差函数，代替了之前为引入层输入的函数。

本文提供了全面的实验证明这些残差网络更容易优化，并且能够从大幅增加的深度中获得准确率增益。在ImageNet数据集上，本文测试了152层的残差网络（比VGG网络深8倍，但具有更低的复杂度），在测试集上达到了3.57%的误差，这个结果赢得了ILSVRC 2015分类任务的第一名。本文也在CIFAR-10数据集上展示了100层和1000层残差网络的分析结果。

网络的深度对于许多视觉识别任务来说是非常重要的，仅仅由于本文实现了极深网络的训练，本文在COCO目标检测数据集上获得了28%的相对提升。深度残差网络也是作者在ImageNet检测、ImageNet定位、COCO检测和COCO分割任务上取得第1名的基础。
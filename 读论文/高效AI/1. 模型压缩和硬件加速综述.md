# 1.【经典综述】模型压缩和硬件加速综述 论文精读

> 原文链接：[Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey](https://ieeexplore.ieee.org/abstract/document/9043731)

## 0. 核心总结

核心是提出了一种**残差结构**，极好地解决了网络精度随着深度的增加而下降的问题，使得可以通过堆叠层数的方式来提升精度。

## 0. 摘要

本文回顾了几种主流的压缩方法，如**紧凑模型**（Compact Model）、**张量分解**（Tensor Decomposition）、**数据量化**（Data Quantization）和**网络稀疏化**（Network Sparsification），并对它们的压缩原理、评价指标、敏感性分析和联合使用进行了说明。

然后，本文回答了如何在神经网络加速器设计中利用这些方法的问题，并展示了最先进的硬件架构。

最后，本文讨论了公平比较、测试工作量、自动压缩、对安全性的影响和对框架/硬件层面的支持性等几个现存的问题，并阐述了该领域的研究热点和可能面临的挑战。

本文的目的是使读者快速构建起神经网络压缩和加速的大图景，清晰地评价各种方法，并为初步的研究提供指引。

## 1. 引言

虽然现在具备高并行处理能力和高内存带宽的GPU很强大，但是它们的功耗和面积却很大，因此在移动设备上使用它们是不现实的，所以人们对能将DNN部署到对资源和能源受限的边缘设备的高效AI的兴趣越来越大。

算法与硬件的交互是神经网络压缩领域的趋势，如下图所示：

![DNN压缩和加速概览图](https://i.imgur.com/MVq5NCC.png)

在算法侧，近些年各式各样的算法被提出，但可以归类为以下4类:

1. 紧凑模型（Compact Model）：通过设计更紧凑的网络结构来减少参数量和计算量，如MobileNet、ShuffleNet、SqueezeNet等。
2. 张量分解（Tensor Decomposition）：通过张量分解来减少参数量和计算量，如CP分解、Tucker分解、TT分解等。
3. 数据量化（Data Quantization）：通过量化权重和激活值来减少参数量和计算量，如INT8、INT4、INT2、二值网络等。
4. 网络稀疏化（Network Sparsification）：通过稀疏化权重和激活值来减少参数量和计算量，如剪枝、稀疏矩阵、稀疏矩阵分解等。

在硬件侧，许多新型的DNN加速器被提出，如TPU、EIE、Eyeriss、DaDianNao等，在这些新型的DNN加速器的设计中，区别于传统加速器只优化硬件结构，它们以一定的精度损失为代价，考虑了多种压缩技术。高层次的算法优化为设计更高效的硬件提供指导，低层次的硬件设计为设计更有效的算法提供反馈，这就是**算法-硬件协同设计**。这种协同设计在现代DNN加速器的设计中无处不在，以提高性能。

本文的所有内容总结于下表，我们的解读文章也按表中所列的章节序号进行章节编排。

![文章内容导引](https://i.imgur.com/Nw36Rnw.png)

## 2. 神经网络的简要预备知识
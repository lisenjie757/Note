# 2.【经典网络】ResNet 论文精读

> 原文链接：[Deep Residual Learning for Image Recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

## 0. 核心总结

核心是提出了一种**残差结构**，极好地解决了网络精度随着深度的增加而下降的问题，使得可以通过堆叠层数的方式来提升精度。

## 1. 摘要

更深的神经网络更难训练，本文提出了一个残差学习框架，以减轻深层神经网络训练的难度。本文将网络的层重构为引入层输入的可学习残差函数，代替了之前为引入层输入的函数。

本文提供了全面的实验证明这些残差网络更容易优化，并且能够从大幅增加的深度中获得准确率增益。在ImageNet数据集上，本文测试了152层的残差网络（比VGG网络深8倍，但具有更低的复杂度），在测试集上达到了3.57%的误差，这个结果赢得了ILSVRC 2015分类任务的第一名。本文也在CIFAR-10数据集上展示了100层和1000层残差网络的分析结果。

网络的深度对于许多视觉识别任务来说是非常重要的，仅仅由于本文实现了极深网络的训练，本文在COCO目标检测数据集上获得了28%的相对提升。深度残差网络也是作者在ImageNet检测、ImageNet定位、COCO检测和COCO分割任务上取得第1名的基础。

## 2. 引言

卷积神经网络的深度是非常重要的，之前在ImageNet数据集上取得领先结果的经典模型都从深度上受益，但是现在深度卷积神经网络正面临臭名昭著的梯度消失/爆炸问题。

虽然这个问题目前可以通过Normalization来解决，但却又出现了退化问题：随着网络深度的增加，精度达到饱和，然后迅速退化，即**更深的网络精度反而下降**。这种退化并不是由过拟合引起的，因为训练误差同样增高，实验结果如下图：

![不同层数卷积神经网络的训练和测试误差](https://i.imgur.com/pnZMQpr.png)

根据朴素的直觉，即使更深的网络的深层部分“什么都不做”，即输出是输入的恒等映射，深层网络起码也不应该比浅层网络的误差高，但这样的实验结果表明目前的优化器找不到这样的解。

因此，本文引入了一个残差块来解决退化问题，如下图所示，通过一个捷径连接（Shortcut Connections）构建输入的恒等映射

![残差块结构示意图](https://i.imgur.com/VFWqkqe.png)